# -*- coding: utf-8 -*-
"""Twitter Sentiment Analysis using Python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZidqspUSi1qssC39XcoA63UfpknU8LA7
"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d kazanova/sentiment140

#Extract copress dataset
from zipfile import ZipFile
dataset="/content/sentiment140.zip"
with ZipFile(dataset,"r") as zip:
  zip.extractall()
  print("The dataset is extracted")

"""importing the dependencies"""

import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

import nltk
nltk.download("stopwords")

#printing sopwords n englist
print(stopwords.words("english"))

"""Data Processing"""

# l oading the dataset into the pandaframe
twitter_data=pd.read_csv("/content/training.1600000.processed.noemoticon.csv",encoding="ISO-8859-1")

#CHECKING THE ROWS AND COLUMNS
twitter_data.shape

twitter_data.head()

twitter_data.columns

column_names=["target","id","date","flag","user","text"]

twitter_data=pd.read_csv("/content/training.1600000.processed.noemoticon.csv",names=column_names,encoding="ISO-8859-1")

twitter_data.head()

twitter_data.columns

#cOnting the number of missing valus in the dataset
twitter_data.isnull().sum()

#Checking the distribution of target columns
twitter_data['target'].value_counts()

"""Convert the target "4" to "1"
"""

twitter_data.replace({"target":{4:1}},inplace=True)

twitter_data["target"].value_counts()

"""0 => Negative Tweet
1 => Positive Tweet

Stemmin is te process of reducing a word into its root word

example : actor,actress,acting = act
"""

port_stem=PorterStemmer()

def stemming(content):
  stemmed_content=re.sub("[^a-zA-Z]"," ",content)
  stemmed_content=stemmed_content.lower()
  stemmed_content=stemmed_content.split()
  stemmed_content=[port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
  stemmed_content= " ".join(stemmed_content)
  return stemmed_content

twitter_data['stemmed_content']=twitter_data['text'].apply(stemming)

twitter_data.head()

X=twitter_data['stemmed_content'].values
y=twitter_data['target'].values

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,stratify=y,random_state=42)

vectorizer=TfidfVectorizer()
X_train=vectorizer.fit_transform(X_train)
X_test=vectorizer.fit_transform(X_test)

model=LogisticRegression()
model.fit(X_train,y_train)

X_train_accuracy=model.predict(X_train)
accuracy_train_data=accuracy_score(y_train,X_train_accuracy)
print("Accuracy of train data : ",accuracy_train_data)

import pickle

pickle.dump(model,open("model.pkl","wb"))

